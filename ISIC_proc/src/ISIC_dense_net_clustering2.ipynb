{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    brightness_range=[0.9, 1.1],\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_generator(\n",
    "    image_directory,\n",
    "    filtered_metadata,\n",
    "    label_encoder,\n",
    "    additional_augmentations,\n",
    "    batch_size=32,\n",
    "    num_classes=8,\n",
    "    augment_data=False,\n",
    "    label=\"diagnosis\",\n",
    "    filter=True,\n",
    "):\n",
    "    num_samples = len(filtered_metadata)\n",
    "\n",
    "    while True:  # Generator loops indefinitely\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = filtered_metadata.iloc[offset : offset + batch_size]\n",
    "\n",
    "            images = []\n",
    "            labels = []\n",
    "            for ix, row in batch_samples.iterrows():\n",
    "                if filter is True and pd.isna(\n",
    "                    row[label]\n",
    "                ):  # Assuming 'label' is the name of your label column\n",
    "                    continue\n",
    "\n",
    "                img_path = os.path.join(image_directory, row[\"isic_id\"] + \".JPG\")\n",
    "                try:\n",
    "                    img = image.load_img(img_path, target_size=(224, 224))\n",
    "                    img = image.img_to_array(img)\n",
    "                    img = K.applications.densenet.preprocess_input(img)\n",
    "\n",
    "                    # Apply LabelEncoder to get the encoded label\n",
    "                    encoded_label = label_encoder.transform([row[label]])[\n",
    "                        0\n",
    "                    ]  # Ensure this matches your label column\n",
    "\n",
    "                    # Check if augmentation is needed for this label\n",
    "                    if augment_data and additional_augmentations[label] > 0:\n",
    "                        augmented_img = img.reshape(\n",
    "                            (1,) + img.shape\n",
    "                        )  # Reshape for data_gen\n",
    "                        for _ in range(additional_augmentations[label]):\n",
    "                            augmented_image = data_gen.flow(\n",
    "                                augmented_img, batch_size=1\n",
    "                            ).next()[0]\n",
    "                            images.append(augmented_image)\n",
    "                            labels.append(\n",
    "                                encoded_label\n",
    "                            )  # Use the same encoded label for augmented images\n",
    "\n",
    "                    # Append the original image and its label\n",
    "                    images.append(img)\n",
    "                    labels.append(encoded_label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {img_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if images:\n",
    "                X_batch = np.array(images)\n",
    "                Y_batch = K.utils.to_categorical(labels, num_classes=num_classes)\n",
    "                yield X_batch, Y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_generator(filtered_metadata, label_encoder, batch_size=32, num_classes=8, label=\"diagnosis\", filter=True):\n",
    "    num_samples = len(filtered_metadata)\n",
    "\n",
    "    while True:\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = filtered_metadata.iloc[offset : offset + batch_size]\n",
    "\n",
    "            labels = []\n",
    "            for ix, row in batch_samples.iterrows():\n",
    "                if filter is True and pd.isna(row[label]):\n",
    "                    continue\n",
    "\n",
    "                # Apply LabelEncoder to get the encoded label\n",
    "                encoded_label = label_encoder.transform([row[label]])[0]\n",
    "\n",
    "                labels.append(encoded_label)\n",
    "\n",
    "            if labels:\n",
    "                Y_batch = K.utils.to_categorical(labels, num_classes=num_classes)\n",
    "                yield Y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentage_of_classes(Y, hot_encoded=False):\n",
    "    if hot_encoded:\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "    counts = Counter(Y)\n",
    "    total = sum(counts.values())\n",
    "    percents = {key: value / total * 100 for key, value in counts.items()}\n",
    "    return percents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, Y, num_classes=8):\n",
    "    X = K.applications.densenet.preprocess_input(X)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(Y)\n",
    "    Y = K.utils.to_categorical(encoded_labels, num_classes=num_classes)\n",
    "\n",
    "    return X, Y, label_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fine_tuned_DenseNet(\n",
    "    input_tensor,\n",
    "    num_classes=8,\n",
    "):\n",
    "    print(\"Getting finetuned DenseNet\")\n",
    "    UNFREEZE_LAYER = -30\n",
    "\n",
    "    # Step 1 load model\n",
    "    base_model = tf.keras.applications.DenseNet201(\n",
    "        include_top=False, weights=\"imagenet\", input_tensor=input_tensor\n",
    "    )\n",
    "\n",
    "    # Step 2 add Custom layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    x = base_model.output\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(1024, activation=\"relu\")(x)\n",
    "\n",
    "    predictions = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Step 3 Compile Model\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    # Step 4 Optional Initail Training\n",
    "    # Step 5 Freeze Layers and set trainable\n",
    "    unfreeze_layer = UNFREEZE_LAYER\n",
    "    for layer in model.layers[:unfreeze_layer]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[unfreeze_layer:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Step 6 Compile Model again\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    # Step 7, Fine tuning done at the return\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(\n",
    "    image_directory,\n",
    "    metadata_path,\n",
    "    label_column,\n",
    "    minimum_count=10,\n",
    "    filter_images=True,\n",
    "    min_count_threshold=0.25,\n",
    "):\n",
    "    # Load and preprocess metadata\n",
    "    metadata = pd.read_csv(metadata_path)\n",
    "    # if not filter_images:\n",
    "    # diagnosis_counts = metadata[label_column].value_counts(dropna=False)\n",
    "    # else:\n",
    "    diagnosis_counts = metadata[label_column].value_counts()\n",
    "\n",
    "    # Filter metadata\n",
    "    if filter_images:\n",
    "        filtered_metadata = metadata[\n",
    "            metadata[label_column].isin(\n",
    "                diagnosis_counts[diagnosis_counts >= minimum_count].index\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        filtered_metadata = metadata\n",
    "\n",
    "    # Fit the LabelEncoder on the filtered labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    if not filter_images:\n",
    "        label_encoder.fit(filtered_metadata[label_column])\n",
    "    else:\n",
    "        label_encoder.fit(filtered_metadata[label_column].dropna())\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    return filtered_metadata, label_encoder, num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_data(y1, preds, mapping):\n",
    "    # Initialize dictionaries to store the results\n",
    "    cluster_stats = {}\n",
    "    totals = {key: 0 for key in range(8)}\n",
    "    max_values = {key: {\"label\": None, \"value\": 0} for key in range(8)}\n",
    "\n",
    "    for val in set(y1):\n",
    "        inds = [i for i in range(len(y1)) if y1[i] == val]\n",
    "        p = preds[inds]\n",
    "        y2 = y1[inds]\n",
    "        counts = dict(Counter(p))\n",
    "\n",
    "        # Store the counts in the cluster_stats dictionary\n",
    "        cluster_stats[val] = counts\n",
    "\n",
    "        # Calculate totals and update max_values for each key (cluster)\n",
    "        for key, value in counts.items():\n",
    "            totals[key] += value\n",
    "            if value > max_values[key][\"value\"]:\n",
    "                max_values[key] = {\"label\": key, \"value\": value}\n",
    "\n",
    "        print(\"Cluster:\", val)\n",
    "        print(\"Counts:\", counts)\n",
    "        # Max value and label\n",
    "        print(\"Max value:\", max(counts.values()))\n",
    "        max_label = max(counts, key=counts.get)\n",
    "        print(\"Max label:\", max_label, mapping[max_label])\n",
    "        percentage = (max(counts.values()) / sum(counts.values())) * 100\n",
    "        percentage = round(percentage, 2)\n",
    "        print(f\"{percentage}% exclusive\")\n",
    "        # Total count\n",
    "        print(\"Total count:\", sum(counts.values()))\n",
    "        print(\"----------------\")\n",
    "\n",
    "    # After processing all clusters, print the aggregated statistics\n",
    "    print(\"Total Counts for Each Cluster:\", totals)\n",
    "    print(\"Maximum Value and Label for Each Cluster:\", max_values)\n",
    "\n",
    "    return cluster_stats, totals, max_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_standard_python() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return False   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        # Probably a standard Python interpreter\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_results(y_data, pred_data, kmeans, label_encoder, num_classes):\n",
    "    y1 = np.argmax(y_data, axis=1)\n",
    "    preds = kmeans.predict(pred_data)\n",
    "\n",
    "    decoded_labels = label_encoder.inverse_transform(preds)\n",
    "\n",
    "    mapping = {}\n",
    "    reverse_mapping = {}\n",
    "\n",
    "    for d, p in zip(decoded_labels, preds):\n",
    "        mapping[d] = p\n",
    "        reverse_mapping[p] = d\n",
    "        if len(mapping) == num_classes:\n",
    "            break\n",
    "\n",
    "    cluster_stats, totals, max_values = view_data(y1, preds, reverse_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = \"/uw/ml_unsuper/ISIC_proc/data/ham10000_metadata_2023-11-27.csv\"\n",
    "densenet_metadata_path = (\n",
    "    \"/uw/ml_unsuper/ISIC_proc/data/challenge-2020-training_metadata_2023-11-28.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" and is_standard_python():\n",
    "    d = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "    data_dir = os.path.join(d, \"data\")\n",
    "else:\n",
    "    data_dir = \"../data\"\n",
    "\n",
    "image_directory = os.path.join(data_dir, \"images\")\n",
    "densenet_image_directory = os.path.join(data_dir, \"images2\")\n",
    "X_path = os.path.join(data_dir, \"X.npy\")\n",
    "Y_path = os.path.join(data_dir, \"Y.npy\")\n",
    "\n",
    "num_classes_path = os.path.join(data_dir, \"num_classes.npy\")\n",
    "save_preds_train = os.path.join(data_dir, \"preds_train.npy\")\n",
    "save_preds_test = os.path.join(data_dir, \"preds_test.npy\")\n",
    "save_preds_X = os.path.join(data_dir, \"preds_X.npy\")\n",
    "\n",
    "\n",
    "save_true_Ytrain = os.path.join(data_dir, \"true_Ytrain.npy\")\n",
    "save_true_Ytest = os.path.join(data_dir, \"true_Ytest.npy\")\n",
    "save_true_Y = os.path.join(data_dir, \"true_Y.npy\")\n",
    "\n",
    "MODEL_NAME = os.path.join(data_dir, \"fine_tuned_model.keras\")\n",
    "densenet_X_path = os.path.join(data_dir, \"densenet_X.npy\")\n",
    "densenet_Y_path = os.path.join(data_dir, \"densenet_Y.npy\")\n",
    "\n",
    "\n",
    "assert os.path.exists(data_dir), \"Data directory not found\"\n",
    "assert os.path.exists(image_directory), \"Image directory not found\"\n",
    "assert os.path.exists(metadata_path), \"Metadata file not found\"\n",
    "assert os.path.exists(densenet_metadata_path), \"Metadata file not found\"\n",
    "assert os.path.exists(densenet_image_directory), \"Image directory not found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_augmentations(filtered_metadata, label_column, min_count_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Calculate the number of additional images needed for each label in a dataset.\n",
    "\n",
    "    Parameters:\n",
    "        filtered_metadata (pandas.DataFrame): The filtered metadata containing the image labels. The applied\n",
    "            filters may include a minimum number of labels and exclusion of NaN labels.\n",
    "        label_column (str): The name of the column in the metadata that contains the image labels.\n",
    "        min_count_threshold (float, optional): The minimum count threshold as a fraction of the maximum count.\n",
    "            Default is 0.25.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the labels and the values are the number of additional images\n",
    "            needed for each label.\n",
    "    \"\"\"\n",
    "    label_counts = filtered_metadata[label_column].value_counts()\n",
    "    max_count = max(label_counts)\n",
    "    minimum_count = round(max_count * min_count_threshold)\n",
    "\n",
    "    additional_images_needed = {\n",
    "        label: max(0, minimum_count - count) for label, count in label_counts.items()\n",
    "    }\n",
    "    return additional_images_needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"diagnosis\"\n",
    "minimum_count = 10\n",
    "filtered_metadata, label_encoder, num_classes = get_metadata(\n",
    "    image_directory, metadata_path, label, minimum_count=minimum_count\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_generator(model, metadata, x_generator, batch_size=32):\n",
    "    num_samples = len(metadata)\n",
    "    predict_steps = np.ceil(num_samples / batch_size)\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    for _ in range(int(predict_steps)):\n",
    "        X_batch, Y_batch = next(x_generator)  # Get both features and labels from the same generator\n",
    "\n",
    "        batch_predictions = model.predict(X_batch)\n",
    "        predictions.extend(batch_predictions)\n",
    "        true_labels.extend(Y_batch)  # Assuming Y_batch is not one-hot encoded; if it is, convert it back\n",
    "\n",
    "    return np.array(predictions), np.array(true_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming filtered_metadata is already prepared\n",
    "train_metadata, test_metadata = train_test_split(\n",
    "    filtered_metadata, test_size=0.2, random_state=42\n",
    ")\n",
    "# Calculate additional augmentations needed\n",
    "\n",
    "label_column = \"diagnosis\"\n",
    "train_augments = calculate_augmentations(train_metadata, label_column)\n",
    "test_augments = calculate_augmentations(test_metadata, label_column)\n",
    "X_augments = calculate_augmentations(filtered_metadata, label_column)\n",
    "\n",
    "train_generator = image_generator(\n",
    "    image_directory,\n",
    "    train_metadata,\n",
    "    label_encoder,\n",
    "    train_augments,\n",
    "    batch_size=32,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "test_generator = image_generator(\n",
    "    image_directory,\n",
    "    test_metadata,\n",
    "    label_encoder,\n",
    "    test_augments,\n",
    "    batch_size=32,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "\n",
    "X_generator = image_generator(\n",
    "    image_directory,\n",
    "    filtered_metadata,\n",
    "    label_encoder,\n",
    "    X_augments,\n",
    "    batch_size=32,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "\n",
    "# Usage\n",
    "\n",
    "\n",
    "Y_generator = label_generator(\n",
    "    filtered_metadata,\n",
    "    label_encoder,\n",
    "    batch_size=32,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "\n",
    "Ytrain_generator = label_generator(\n",
    "    train_metadata,\n",
    "    label_encoder,\n",
    "    batch_size=32,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "\n",
    "Ytest_generator = label_generator(\n",
    "    test_metadata,\n",
    "    label_encoder,\n",
    "    batch_size=32,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "force_load_model = False\n",
    "EPOCHS = 4\n",
    "\n",
    "if not os.path.exists(MODEL_NAME) or force_load_model == True:\n",
    "    print(\"Fine tuning the model. This will take a while. Please wait.\")\n",
    "    print(\"Loading training data for DenseNet\")\n",
    "\n",
    "    # Get metadata and initialize generators for training and validation\n",
    "\n",
    "    densenet_metadata, label_encoder, num_classes = get_metadata(\n",
    "        densenet_image_directory,\n",
    "        densenet_metadata_path,  # Ensure this is the correct path\n",
    "        label,\n",
    "        minimum_count=minimum_count,\n",
    "        filter_images=False,\n",
    "    )\n",
    "    densenet_augments = calculate_augmentations(\n",
    "        densenet_metadata, label, min_count_threshold=0.25\n",
    "    )\n",
    "\n",
    "    densenetX_generator = image_generator(\n",
    "        densenet_image_directory,\n",
    "        densenet_metadata,  # Assuming this is for training\n",
    "        label_encoder,\n",
    "        {},\n",
    "        batch_size=32,\n",
    "        num_classes=num_classes,\n",
    "        augment_data=False,\n",
    "        filter=False,\n",
    "    )\n",
    "\n",
    "    # If you have separate validation metadata\n",
    "    # validation_generator = ...\n",
    "\n",
    "    print(\"Fine-tuning DenseNet ...\")\n",
    "    input_tensor = K.Input(shape=(224, 224, 3))\n",
    "    model = get_fine_tuned_DenseNet(input_tensor, num_classes=num_classes)\n",
    "\n",
    "    print(\"Training pre-trained model\")\n",
    "    model.fit(\n",
    "        densenetX_generator,\n",
    "        epochs=EPOCHS,\n",
    "        steps_per_epoch=len(densenet_metadata) // 32,\n",
    "    )\n",
    "    model.save(MODEL_NAME)  # Use model.save instead of np.save for models\n",
    "\n",
    "else:\n",
    "    print(\"Loading pre-trained model from local storage\")\n",
    "    model = load_model(MODEL_NAME)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_load_preds = False\n",
    "\n",
    "if not os.path.isfile(save_preds_train) or force_load_preds == True:\n",
    "    # preds_train, true_Ytrain = predict_generator(model, filtered_metadata, X_generator, Y_generator)\n",
    "    # preds_test, true_Ytest = predict_generator(model, filtered_metadata, X_generator, Y_generator)\n",
    "    # preds_X, true_Y = predict_generator(model, filtered_metadata, X_generator, Y_generator)\n",
    "\n",
    "\n",
    "    preds_train, true_Ytrain = predict_generator(model, train_metadata, train_generator)\n",
    "    preds_test, true_Ytest = predict_generator(model, test_metadata, test_generator)\n",
    "    preds_X, true_Y = predict_generator(model, filtered_metadata, X_generator)\n",
    "\n",
    "\n",
    "    np.save(save_preds_train, preds_train)\n",
    "    np.save(save_preds_test, preds_test)\n",
    "    np.save(save_preds_X, preds_X)\n",
    "\n",
    "    np.save(save_true_Ytrain, true_Ytrain)\n",
    "    np.save(save_true_Ytest, true_Ytest)\n",
    "    np.save(save_true_Y, true_Y)\n",
    "else:\n",
    "    print(\"Loading precomputed predictions\")\n",
    "\n",
    "    preds_train = np.load(save_preds_train)\n",
    "    preds_test = np.load(save_preds_test)\n",
    "    preds_X = np.load(save_preds_X)\n",
    "\n",
    "    true_Ytrain = np.load(save_true_Ytrain)\n",
    "    true_Ytest = np.load(save_true_Ytest)\n",
    "    true_Y = np.load(save_true_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Usage\n",
    "# X_generator = image_generator(\n",
    "#     image_directory,\n",
    "#     filtered_metadata,\n",
    "#     label_encoder,\n",
    "#     X_augments,\n",
    "#     batch_size=32,\n",
    "#     num_classes=num_classes,\n",
    "# )\n",
    "\n",
    "# Y_generator = label_generator(\n",
    "#     filtered_metadata,\n",
    "#     label_encoder,\n",
    "#     batch_size=32,\n",
    "#     num_classes=num_classes,\n",
    "# )\n",
    "\n",
    "# preds_X, true_Y = predict_generator(model, filtered_metadata, X_generator, Y_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=num_classes,\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    algorithm=\"elkan\",\n",
    "    tol=0.000001,\n",
    ").fit(preds_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_results(true_Ytest, preds_test, kmeans, label_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(\n",
    "    n_clusters=num_classes,\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    algorithm=\"elkan\",\n",
    "    tol=0.000001,\n",
    ").fit(preds_X)\n",
    "view_results(true_Y, preds_X, kmeans, label_encoder)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_local_saved_data():\n",
    "    try:\n",
    "        remove_these = [\n",
    "            X_path,\n",
    "            Y_path,\n",
    "            num_classes_path,\n",
    "            save_preds_train,\n",
    "            save_preds_test,\n",
    "            save_preds_X,\n",
    "            \n",
    "            MODEL_NAME,\n",
    "        ]\n",
    "    except NameError:\n",
    "        print(\"No local saved data to clear or local saved data is incomplete.\")\n",
    "        return\n",
    "    for r in remove_these:\n",
    "        if os.path.isfile(r):\n",
    "            try:\n",
    "                os.remove(r)\n",
    "            except OSError:\n",
    "                print(f\"Failed to remove {r}\")\n",
    "        else:\n",
    "            print(f\"{r} does not exist.\")\n",
    "\n",
    "\n",
    "# ask user if they want to remove the data\n",
    "remove_data = input(\"Do you want to remove the data? (y/n): \")\n",
    "if remove_data.lower() == \"y\":\n",
    "    clear_local_saved_data()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
